{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to illustrate my observations on the GPT2 model, i have split the code into important individual components used in the github repo of this project (https://github.com/openai/gpt-2). \n",
    "\n",
    "This is also a step by step approach, building towards the final LM inference output, to get a better understanding of how this works and generate use cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json\n",
    "import tensorflow as tf\n",
    "import regex as re\n",
    "import time\n",
    "import numpy as np\n",
    "sys.path.append(\"src/\")\n",
    "import encoder, sample, model\n",
    "\n",
    "model_name=\"117M\"\n",
    "cache = {}\n",
    "\n",
    "def load_encoder_json():\n",
    "    with open(os.path.join('models', model_name, 'encoder.json'), 'r') as f:\n",
    "        encoder_json = json.load(f)\n",
    "    return encoder_json\n",
    "def load_bpe_merges():\n",
    "    with open(os.path.join('models', model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    return [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "def overwrite_model_params(hparams):\n",
    "    with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the String Encoder\n",
    "\n",
    "Encoding an input sentence involves the following steps\n",
    "1. find word tokens in a given sentence (\"I love apples\" --> [\"i\", \" love\", \" apples\"]), notice the space before words\n",
    "2. for each character in a given token, use the mapping logic (refer bytes_to_unicode in encode.py) to convert them back to their corresponding character, instead of using ord()\n",
    "3. apply bpe on the tokens obtained from 2, these bpe tokens \n",
    "4. split the bpe tokens (.split(\" \")) and get the index for each bpe token splits using the enoder_json, this is just the vocabulary (word2index) used for training the GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load encoder json, vocabulary of 50,257 tokens, token to index, decoder for index2word\n",
    "encoder_json = load_encoder_json()\n",
    "decoder_json = {v:k for k,v in encoder_json.items()}\n",
    "# load bpe data, some merge map based on character frequency (check this)\n",
    "bpe_merges = load_bpe_merges()\n",
    "bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "\n",
    "# encoder to get bytes and decoder to get characters from bytes, from the unicode map (refer bytes_to_unicode in encode.py)\n",
    "byte_encoder = encoder.bytes_to_unicode()\n",
    "byte_decoder = {v:k for k, v in byte_encoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful function for encoding and decoding word tokens\n",
    "def bpe(token):\n",
    "    \"\"\"function to get bpe token from a regular token, taken from encoder.py \"\"\"\n",
    "    if token in cache:\n",
    "        return cache[token]\n",
    "    word = tuple(token)\n",
    "    pairs = encoder.get_pairs(word)\n",
    "    if not pairs:\n",
    "        return token\n",
    "    while True:\n",
    "        bigram = min(pairs, key = lambda pair: bpe_ranks.get(pair, float('inf')))\n",
    "        if bigram not in bpe_ranks:\n",
    "            break\n",
    "        first, second = bigram\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "\n",
    "            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                new_word.append(first+second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = encoder.get_pairs(word)\n",
    "    word = ' '.join(word)\n",
    "    cache[token] = word\n",
    "    return word\n",
    "\n",
    "def get_bpe_tokens(text):\n",
    "    \"\"\"function to get bpe tokens from a give text\"\"\"\n",
    "    # some complex regex to select individual tokens\n",
    "    pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "    bpe_tokens = []\n",
    "    for token in re.findall(pat, text):\n",
    "        token = ''.join(byte_encoder[b] for b in token.encode('utf-8'))\n",
    "        bpe_tokens.extend(encoder_json[bpe_token] for bpe_token in bpe(token).split(' '))\n",
    "    return bpe_tokens\n",
    "\n",
    "def decode_output(tokens):\n",
    "    \"\"\"decode output from the LM, convert word indices to text using the decoder json and byte decoders\"\"\"\n",
    "    text = ''.join([decoder_json[token] for token in tokens])\n",
    "    text = bytearray([byte_decoder[c] for c in text]).decode('utf-8', errors='replace')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('!', 0),\n",
       " ('\"', 1),\n",
       " ('#', 2),\n",
       " ('$', 3),\n",
       " ('%', 4),\n",
       " ('&', 5),\n",
       " (\"'\", 6),\n",
       " ('(', 7),\n",
       " (')', 8),\n",
       " ('*', 9),\n",
       " ('+', 10),\n",
       " (',', 11),\n",
       " ('-', 12),\n",
       " ('.', 13),\n",
       " ('/', 14),\n",
       " ('0', 15),\n",
       " ('1', 16),\n",
       " ('2', 17),\n",
       " ('3', 18),\n",
       " ('4', 19)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e for e in encoder_json.items()][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 1 # number of samples\n",
    "batch_size = 1 \n",
    "temperature = 1 # not sure what this is, need to check\n",
    "top_k = 40 # next word is selected from top k predictions of LM, uses tf.multinomial to pick one from a sample of topk\n",
    "hparams = model.default_hparams() # model params\n",
    "# overwrite\n",
    "overwrite_model_params(hparams)\n",
    "length = hparams.n_ctx // 2 # sentence length to generate, this value is 512 by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language model will generate the rest of the sequence\n",
    "text = \"i love football\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Sequence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/117M/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# this intial part of this code involves loading the model and tokens into the tf graph\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    # input context\n",
    "    context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    \n",
    "    # this sample sequence uses tf.multinomial distribution to select next words from top_k to generate sequences\n",
    "    output = sample.sample_sequence(\n",
    "        hparams=hparams, length=length,\n",
    "        context=context,\n",
    "        batch_size=batch_size,\n",
    "        temperature=temperature, top_k=top_k\n",
    "    )\n",
    "    \n",
    "    # load model \n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))\n",
    "    saver.restore(sess, ckpt)\n",
    "    \n",
    "    # get model context tokens\n",
    "    context_tokens = get_bpe_tokens(text)\n",
    "    \n",
    "    # run the session and generate samples based on the input context\n",
    "    generated = 0    \n",
    "    for _ in range(nsamples // batch_size):\n",
    "        # output here is of length 'length'\n",
    "        out = sess.run(output, feed_dict={\n",
    "            context: [context_tokens for _ in range(batch_size)]\n",
    "        })[:, len(context_tokens):]\n",
    "        \n",
    "        # decode the output\n",
    "        for i in range(batch_size):\n",
    "            generated += 1\n",
    "            text =decode_output(out[i])\n",
    "            print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "            print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Next Word Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/117M/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "text = \"the new york times says\"\n",
    "\n",
    "def step(hparams, tokens, past=None):\n",
    "    lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n",
    "\n",
    "    logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
    "    presents = lm_output['present']\n",
    "    presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))\n",
    "    return {\n",
    "        'logits': logits,\n",
    "        'presents': presents,\n",
    "    }\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    # input context\n",
    "    context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    \n",
    "    \n",
    "    # this sample sequence uses tf.multinomial distribution to select next words from top_k to generate sequences\n",
    "    output = step(hparams=hparams,tokens=context)\n",
    "    # load model \n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))\n",
    "    saver.restore(sess, ckpt)\n",
    "    \n",
    "    # get model context tokens\n",
    "    context_tokens = get_bpe_tokens(text)\n",
    "    \n",
    "    # run the session and get next word probabilities\n",
    "    # output here is of length 'length'\n",
    "    out = sess.run(output, feed_dict={\n",
    "        context: [context_tokens for _ in range(batch_size)]\n",
    "    })\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that -79.419\n",
      " it -79.535\n",
      ": -79.539\n",
      ", -79.846\n",
      " the -80.011\n",
      " \" -80.192\n",
      " they -80.704\n",
      " ' -80.77\n",
      " to -80.941\n",
      " he -80.979\n",
      " this -81.081\n",
      " we -81.083\n",
      " a -81.189\n",
      " you -81.2\n",
      " I -81.254\n",
      " there -81.323\n",
      ". -81.481\n",
      ") -81.718\n",
      " something -81.806\n",
      " no -82.083\n",
      " she -82.249\n",
      " things -82.312\n",
      " its -82.361\n",
      "). -82.397\n",
      " nothing -82.482\n",
      " more -82.488\n",
      " in -82.565\n",
      " all -82.616\n",
      " so -82.964\n",
      " about -83.127\n",
      " on -83.127\n",
      " everything -83.15\n",
      " if -83.182\n",
      "... -83.195\n",
      "\" -83.22\n",
      "\n",
      " -83.285\n",
      " not -83.303\n",
      ",\" -83.382\n",
      " people -83.388\n",
      " what -83.393\n",
      " some -83.408\n",
      " these -83.414\n",
      " of -83.419\n",
      " his -83.423\n",
      " one -83.432\n",
      " yes -83.432\n",
      " at -83.44\n",
      " and -83.447\n",
      "), -83.45\n",
      " an -83.466\n",
      ".\" -83.476\n",
      " - -83.52\n",
      ".) -83.525\n",
      " : -83.538\n",
      " their -83.542\n",
      " for -83.562\n",
      " your -83.601\n",
      " ( -83.617\n",
      "] -83.673\n",
      " my -83.719\n",
      " as -83.729\n",
      " much -83.733\n",
      "): -83.743\n",
      " anything -83.781\n",
      " our -83.785\n",
      "… -83.837\n",
      "- -83.872\n",
      " is -83.9\n",
      " here -83.91\n",
      " with -83.923\n",
      " many -83.945\n",
      " i -83.952\n",
      " – -83.963\n",
      " just -83.97\n",
      " how -84.01\n",
      " good -84.023\n",
      "; -84.09\n",
      " hello -84.191\n",
      " are -84.197\n",
      " [ -84.205\n",
      " goodbye -84.227\n",
      " don -84.256\n",
      " only -84.28\n",
      " little -84.302\n",
      " ... -84.308\n",
      " those -84.322\n",
      " when -84.354\n",
      " Y -84.362\n",
      " everyone -84.386\n",
      " otherwise -84.39\n",
      " have -84.425\n",
      " very -84.466\n",
      " most -84.47\n",
      "' -84.492\n",
      " her -84.504\n",
      " y -84.543\n",
      " New -84.564\n",
      " well -84.589\n",
      " do -84.591\n",
      " too -84.619\n"
     ]
    }
   ],
   "source": [
    "probs = out['logits']\n",
    "sentence_length = len(text.split())\n",
    "sorted_probs = np.argsort(-probs[0,sentence_length,:])\n",
    "for i in sorted_probs[:100]:\n",
    "    print(decode_output([i]),round(probs[0,sentence_length,:][i],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
