{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to illustrate my observations on the GPT2 model, i have split the code into important individual components used in the github repo of this project (https://github.com/openai/gpt-2). \n",
    "\n",
    "This is also a step by step approach, building towards the final LM inference output, to get a better understanding of how this works and generate use cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.virtualenvs/notebook/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys, os, json\n",
    "import tensorflow as tf\n",
    "import regex as re\n",
    "sys.path.append(\"src/\")\n",
    "import encoder, sample, model\n",
    "\n",
    "model_name=\"117M\"\n",
    "cache = {}\n",
    "\n",
    "def load_encoder_json():\n",
    "    with open(os.path.join('models', model_name, 'encoder.json'), 'r') as f:\n",
    "        encoder_json = json.load(f)\n",
    "    return encoder_json\n",
    "def load_bpe_merges():\n",
    "    with open(os.path.join('models', model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    return [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "def overwrite_model_params(hparams):\n",
    "    with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Encoder\n",
    "\n",
    "Encoding an input sentence involves the following steps\n",
    "1. find word tokens in a given sentence (\"I love apples\" --> [\"i\", \" love\", \" apples\"]), notice the space before words\n",
    "2. for each character in a given token, use the mapping logic (refer bytes_to_unicode in encode.py) to convert them back to their corresponding character, instead of using ord()\n",
    "3. apply bpe on the tokens obtained from 2, these bpe tokens \n",
    "4. split the bpe tokens (.split(\" \")) and get the index for each bpe token splits using the enoder_json, this is just the vocabulary (word2index) used for training the GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load encoder json, vocabulary of 50,257 tokens, token to index, decoder for index2word\n",
    "encoder_json = load_encoder_json()\n",
    "decoder_json = {v:k for k,v in encoder_json.items()}\n",
    "# load bpe data, some merge map based on character frequency (check this)\n",
    "bpe_merges = load_bpe_merges()\n",
    "bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "\n",
    "# encoder to get bytes and decoder to get characters from bytes, from the unicode map (refer bytes_to_unicode in encode.py)\n",
    "byte_encoder = encoder.bytes_to_unicode()\n",
    "byte_decoder = {v:k for k, v in byte_encoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful function for encoding and decoding word tokens\n",
    "def bpe(token):\n",
    "    \"\"\"function to get bpe token from a regular token, taken from encoder.py \"\"\"\n",
    "    if token in cache:\n",
    "        return cache[token]\n",
    "    word = tuple(token)\n",
    "    pairs = encoder.get_pairs(word)\n",
    "    if not pairs:\n",
    "        return token\n",
    "    while True:\n",
    "        bigram = min(pairs, key = lambda pair: bpe_ranks.get(pair, float('inf')))\n",
    "        if bigram not in bpe_ranks:\n",
    "            break\n",
    "        first, second = bigram\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "\n",
    "            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                new_word.append(first+second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = encoder.get_pairs(word)\n",
    "    word = ' '.join(word)\n",
    "    cache[token] = word\n",
    "    return word\n",
    "\n",
    "def get_bpe_tokens(text):\n",
    "    \"\"\"function to get bpe tokens from a give text\"\"\"\n",
    "    # some complex regex to select individual tokens\n",
    "    pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "    bpe_tokens = []\n",
    "    for token in re.findall(pat, text):\n",
    "        token = ''.join(byte_encoder[b] for b in token.encode('utf-8'))\n",
    "        bpe_tokens.extend(encoder_json[bpe_token] for bpe_token in bpe(token).split(' '))\n",
    "    return bpe_tokens\n",
    "\n",
    "def decode_output(tokens):\n",
    "    \"\"\"decode output from the LM, convert word indices to text using the decoder json and byte decoders\"\"\"\n",
    "    text = ''.join([decoder_json[token] for token in tokens])\n",
    "    text = bytearray([byte_decoder[c] for c in text]).decode('utf-8', errors='replace')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('!', 0),\n",
       " ('\"', 1),\n",
       " ('#', 2),\n",
       " ('$', 3),\n",
       " ('%', 4),\n",
       " ('&', 5),\n",
       " (\"'\", 6),\n",
       " ('(', 7),\n",
       " (')', 8),\n",
       " ('*', 9),\n",
       " ('+', 10),\n",
       " (',', 11),\n",
       " ('-', 12),\n",
       " ('.', 13),\n",
       " ('/', 14),\n",
       " ('0', 15),\n",
       " ('1', 16),\n",
       " ('2', 17),\n",
       " ('3', 18),\n",
       " ('4', 19)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e for e in encoder_json.items()][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 1 # number of samples\n",
    "batch_size = 1 \n",
    "temperature = 1 # not sure what this is, need to check\n",
    "top_k = 40 # next word is selected from top k predictions of LM, uses tf.multinomial to pick one from a sample of topk\n",
    "hparams = model.default_hparams() # model params\n",
    "# overwrite\n",
    "overwrite_model_params(hparams)\n",
    "length = hparams.n_ctx // 2 # sentence length to generate, this value is 512 by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i love football\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Sequence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/117M/model.ckpt\n",
      "======================================== SAMPLE 1 ========================================\n",
      "! The same goes for the rest of us.\n",
      "\n",
      "Boris will take him to the London game where we watch him play for a very short amount of time.\n",
      "\n",
      "And you just know what happens to him? He's going to have two more seasons at a rate of around Â£20 million.\n",
      "\n",
      "You know what? He gets out of this job.<|endoftext|>A few weeks ago I posted about the problem with the NFS in my article \"How to use DAGs for storage on S/M files\". So I thought the following would be a good place to share with you the first step in what I've learned.\n",
      "\n",
      "How do DAGs work to store a file on a partition of a machine?\n",
      "\n",
      "A DAG needs some RAM to store the file in. For this we have to first decide on the \"storage\" type. In the example above, we have a file system partition. DAGs can store a large number of hard disks in the drive (a few thousand are used all the time. The partition contains the filesystems of the filesystem for which we want DAGs to be able to store the data) and that allows them to have a large amount of disk space when the file system is not needed.\n",
      "\n",
      "A DAG must be set in RAM.\n",
      "\n",
      "A DAG needs a RAM to store the file in. For this we have to first decide on the \"storage\" type. In this example, we have a file system partition. DAGs can store a large number of hard disks in the drive (a few thousand are used all the time. The partition contains the filesystems of the filesystem for which we want DAGs to be able to store the data) and that allows them to have a large amount of disk space when the file system is not needed. In the following example, we have a storage system partition that can hold enough hard disks (1,100,000) for any file system file with 128 bytes of files.\n",
      "\n",
      "A DAG needs 4GB of RAM.\n",
      "\n",
      "Now, let us move on to this DAGs idea - the idea was introduced by Martin Osterlund, now at Rambler. I also love the idea of building a DAG with the memory of the partitions in RAM. However there are a lot of technical details that I don't know of that you should know about here because I'm not actually able to understand them all. There is only one thing you really want with D\n"
     ]
    }
   ],
   "source": [
    "# this intial part of this code involves loading the model and tokens into the tf graph\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    # input context\n",
    "    context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    \n",
    "    # this sample sequence uses tf.multinomial distribution to select next words from top_k to generate sequences\n",
    "    output = sample.sample_sequence(\n",
    "        hparams=hparams, length=length,\n",
    "        context=context,\n",
    "        batch_size=batch_size,\n",
    "        temperature=temperature, top_k=top_k\n",
    "    )\n",
    "    \n",
    "    # load model \n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))\n",
    "    saver.restore(sess, ckpt)\n",
    "    \n",
    "    # get model context tokens\n",
    "    context_tokens = get_bpe_tokens(text)\n",
    "    \n",
    "    # run the session and generate samples based on the input context\n",
    "    generated = 0    \n",
    "    for _ in range(nsamples // batch_size):\n",
    "        # output here is of length 'length'\n",
    "        out = sess.run(output, feed_dict={\n",
    "            context: [context_tokens for _ in range(batch_size)]\n",
    "        })[:, len(context_tokens):]\n",
    "        \n",
    "        # decode the output\n",
    "        for i in range(batch_size):\n",
    "            generated += 1\n",
    "            text =decode_output(out[i])\n",
    "            print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "            print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
